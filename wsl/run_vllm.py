import os
from modelscope import snapshot_download
import requests
import json

# ======================
# é…ç½®æ¨¡åž‹
# ======================
LLM_MODEL = "Qwen/Qwen2.5-0.5B-Instruct"
VLM_MODEL = "OpenBMB/MiniCPM-V-2_6-int4"  # æˆ– MiniCPM-V-2_6ï¼ˆéžé‡åŒ–ï¼‰

LLM_DIR = f"/root/models/{LLM_MODEL.replace('/', '_')}"
VLM_DIR = f"/root/models/{VLM_MODEL.replace('/', '_')}"

# ======================
# ä¸‹è½½æ¨¡åž‹ï¼ˆModelScopeï¼‰
# ======================
def download_models():
    for name, path in [(LLM_MODEL, LLM_DIR), (VLM_MODEL, VLM_DIR)]:
        if not os.path.exists(path) or not os.listdir(path):
            print(f"ðŸ“¥ ä¸‹è½½æ¨¡åž‹: {name}")
            os.makedirs(path, exist_ok=True)
            snapshot_download(model_id=name, local_dir=path)
        else:
            print(f"âœ… æ¨¡åž‹å·²å­˜åœ¨: {path}")
download_models()
print("\nðŸš€ å¯åŠ¨ LLM æœåŠ¡ (ç«¯å£ 8000)...")
llm_cmd = (
    f"python -m vllm.entrypoints.openai.api_server "
    f"--model {LLM_DIR} "
    f"--host 0.0.0.0 --port 8000 "
    f"--dtype bfloat16 "
    f"--gpu-memory-utilization 0.5"  # é™ä½Žå†…å­˜åˆ©ç”¨çŽ‡
)

print("ðŸš€ å¯åŠ¨ VLM æœåŠ¡ (ç«¯å£ 8001)...")
vlm_cmd = (
    f"python -m vllm.entrypoints.openai.api_server "
    f"--model {VLM_DIR} "
    f"--host 0.0.0.0 --port 8001 "
    f"--dtype bfloat16 "
    f"--trust-remote-code "  # âš ï¸ å…³é”®å‚æ•°ï¼
    f"--gpu-memory-utilization 0.5"  # é™ä½Žå†…å­˜åˆ©ç”¨çŽ‡
)

print("\nðŸŒ è®¿é—®åœ°å€:")
print(f"   - LLM: http://localhost:8000/v1")
print(f"   - VLM: http://localhost:8001/v1")

# åŽå°å¯åŠ¨ VLMï¼Œå‰å°è¿è¡Œ LLM
os.system(f"nohup {vlm_cmd} > vlm.log 2>&1 &")
os.system(llm_cmd)

