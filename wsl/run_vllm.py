import os
from modelscope import snapshot_download

# ======================
# é…ç½®æ¨¡åž‹
# ======================
LLM_MODEL = "Qwen/Qwen2.5-0.5B"
VLM_MODEL = "OpenBMB/MiniCPM-V-2_6-int4"  # æˆ– MiniCPM-V-2_6ï¼ˆéžé‡åŒ–ï¼‰

LLM_DIR = f"./models/{LLM_MODEL.replace('/', '_')}"
VLM_DIR = f"./models/{VLM_MODEL.replace('/', '_')}"

# ======================
# ä¸‹è½½æ¨¡åž‹ï¼ˆModelScopeï¼‰
# ======================
for name, path in [(LLM_MODEL, LLM_DIR), (VLM_MODEL, VLM_DIR)]:
    if not os.path.exists(path) or not os.listdir(path):
        print(f"ðŸ“¥ ä¸‹è½½æ¨¡åž‹: {name}")
        os.makedirs(path, exist_ok=True)
        snapshot_download(model_id=name, local_dir=path)
    else:
        print(f"âœ… æ¨¡åž‹å·²å­˜åœ¨: {path}")

# ======================
# å¯åŠ¨æœåŠ¡
# ======================
print("\nðŸš€ å¯åŠ¨ LLM æœåŠ¡ (ç«¯å£ 8000)...")
llm_cmd = (
    f"python -m vllm.entrypoints.openai.api_server "
    f"--model {LLM_DIR} "
    f"--host 0.0.0.0 --port 8000 "
    f"--dtype bfloat16"
)

print("ðŸš€ å¯åŠ¨ VLM æœåŠ¡ (ç«¯å£ 8001)...")
vlm_cmd = (
    f"python -m vllm.entrypoints.openai.api_server "
    f"--model {VLM_DIR} "
    f"--host 0.0.0.0 --port 8001 "
    f"--dtype bfloat16 "
    f"--trust-remote-code"  # âš ï¸ å…³é”®å‚æ•°ï¼
)

print("\nðŸŒ è®¿é—®åœ°å€:")
print(f"   - LLM: http://localhost:8000/v1")
print(f"   - VLM: http://localhost:8001/v1")

# åŽå°å¯åŠ¨ VLMï¼Œå‰å°è¿è¡Œ LLM
os.system(f"nohup {vlm_cmd} > vlm.log 2>&1 &")
os.system(llm_cmd)