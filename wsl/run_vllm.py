import os
import subprocess
import signal
import time
from modelscope import snapshot_download

# ======================
# é…ç½®æ¨¡å‹
# ======================
LLM_MODEL = "OpenBMB/MiniCPM4-0.5B-QAT-Int4-GPTQ-format"
VLM_MODEL = "OpenBMB/MiniCPM-V-2_6-int4"  # æˆ– MiniCPM-V-2_6ï¼ˆéé‡åŒ–ï¼‰

LLM_DIR = f"/root/models/{LLM_MODEL.replace('/', '_')}"
VLM_DIR = f"/root/models/{VLM_MODEL.replace('/', '_')}"

# å­˜å‚¨è¿›ç¨‹å¯¹è±¡
processes = []

# ======================
# ä¸‹è½½æ¨¡å‹ï¼ˆModelScopeï¼‰
# ======================
def download_models():
    for name, path in [(LLM_MODEL, LLM_DIR), (VLM_MODEL, VLM_DIR)]:
        if not os.path.exists(path) or not os.listdir(path):
            print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {name}")
            os.makedirs(path, exist_ok=True)
            snapshot_download(model_id=name, local_dir=path)
        else:
            print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {path}")

def start_process(cmd, description):
    print(f"ğŸš€ å¯åŠ¨ {description}...")
    process = subprocess.Popen(cmd, shell=True, preexec_fn=os.setsid)
    processes.append(process)
    return process

def cleanup_processes():
    print("\nğŸ”„ æ¸…ç†è¿›ç¨‹...")
    for process in processes:
        try:
            os.killpg(os.getpgid(process.pid), signal.SIGTERM)
            process.wait(timeout=5)
        except:
            try:
                os.killpg(os.getpgid(process.pid), signal.SIGKILL)
            except:
                pass
    print("âœ… æ‰€æœ‰è¿›ç¨‹å·²æ¸…ç†å®Œæ¯•")

def signal_handler(sig, frame):
    print("\nâš ï¸  æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...")
    cleanup_processes()
    exit(0)

if __name__ == "__main__":
    # æ³¨å†Œä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    download_models()
    
    llm_cmd = (
        f"python -m vllm.entrypoints.openai.api_server "
        f"--model {LLM_DIR} "
        f"--host 0.0.0.0 --port 8000 "
        f"--quantization gptq_marlin "
        f"--trust-remote-code "
        f"--dtype bfloat16 "
        f"--gpu-memory-utilization 0.8 "
        f"--max-num-batched-tokens 32768"
    )
    
    vlm_cmd = (
        f"python -m vllm.entrypoints.openai.api_server "
        f"--model {VLM_DIR} "
        f"--host 0.0.0.0 --port 8001 "
        f"--dtype bfloat16 "                 # RTX 4060 æ›´é€‚åˆ float16
        f"--trust-remote-code "
        f"--gpu-memory-utilization 0.5 "
        f"--max-model-len 1024 "            # è¿›ä¸€æ­¥ç¼©çŸ­
    )
    
    print("\nğŸŒ è®¿é—®åœ°å€:")
    print(f"   - LLM: http://localhost:8000/v1")
    print(f"   - VLM: http://localhost:8001/v1")
    
 
    llm_process = start_process(llm_cmd, "LLM æœåŠ¡ (ç«¯å£ 8000)")
    
    print("\nâœ… æ‰€æœ‰æœåŠ¡å·²å¯åŠ¨")
    print("ğŸ“ æŒ‰ Ctrl+C åœæ­¢æ‰€æœ‰æœåŠ¡...")
    
    # ç­‰å¾…è¿›ç¨‹ç»“æŸ
    try:
        for process in processes:
            process.wait()
    except KeyboardInterrupt:
        cleanup_processes()